# Domain-Adaptive LLM Fine-Tuning for Enterprise Policy QA

A production-grade domain-adaptive fine-tuning pipeline that specializes a pretrained Llama-3.2-1B model for enterprise HR Leave & Benefits policy reasoning, using synthetic instruction generation, quality gating, and LoRA-based PEFT training â€” without retrieval at inference time.

This project demonstrates how small, high-quality policy data can meaningfully alter LLM behavior, improving factual grounding and hallucination control for compliance-sensitive domains.

## ğŸš€ What This Project Does

- ğŸ“„ **Parses enterprise policy PDFs** into structured, context-aware chunks
- ğŸ§  **Generates synthetic instructionâ€“response pairs** using strict policy-grounded prompts
- ğŸ§¹  **Filters low-quality supervision** via heuristic + LLM-based quality gating
- ğŸ§ª  **Fine-tunes a pretrained LLM** using **LoRA (PEFT) + 4-bit quantization**
- âš–ï¸ **Trains hallucination-aware behavior**, enforcing â€œNot specified in the provided excerptâ€
- ğŸ” **Evaluates behavioral divergence** vs. the base model through A/B testing
- ğŸ§© **Deploys adapters locally** using Ollama for reproducible inference

---
## ğŸ’¡ Why This Matters

- Enterprise policies require precision, not creativity
- Base LLMs hallucinate plausible but incorrect policy details
- RAG alone does not fix behavioral priors
- This project shows how fine-tuning reshapes next-token probabilities so the model:
   - answers only what is stated
   - refuses confidently when information is missing
   - internalizes policy structure instead of retrieving it at runtime
- This pattern generalizes to:
   - HR & benefits
   - Legal & compliance
   - Internal SOPs
   - Financial / regulatory documents
---
ğŸ“„ Data & Document Policy

> âš ï¸ Note: This repository does not include source documents or generated training artifacts.  
> Users are expected to supply their own documents and reproduce the pipeline locally or in cloud environments.

---
## ğŸ—ï¸ Architecture Diagram

**Important distinction**

- Training time â†’ document â†’ data â†’ weights
- Inference time â†’ no retrieval, no vector store
  
```mermaid
flowchart LR
    PDF["ğŸ“„ Policy PDF"]

    subgraph Data["ğŸ“¦ Data Generation"]
        Parse["ğŸ” Parse + Chunk<br/>(Docling)"]
        Context["ğŸ§  Contextualize Chunks"]
        Synth["âœï¸ Synthetic Q/A Generation<br/>(Policy-Constrained Prompt)"]
        Pre["ğŸ§¹ Preprocessing"]
        Quality["âš–ï¸ Data Quality Gating<br/>(Heuristics + LLM Judge)"]
    end

    subgraph Train["ğŸ§  Model Training"]
        Base["ğŸ¤– Base LLM<br/>(Llama-3.2-1B)"]
        LoRA["ğŸ§© LoRA PEFT<br/>(r=256, Î±=512)"]
        SFT["ğŸ”¥ Supervised Fine-Tuning"]
    end

    subgraph Deploy["ğŸš€ Deployment"]
        Adapter["ğŸ“¦ LoRA Adapter"]
        Ollama["ğŸ–¥ï¸ Ollama Runtime"]
    end

    PDF --> Parse --> Context --> Synth --> Pre --> Quality
    Quality -->|"124 high-quality samples"| SFT
    Base --> SFT --> LoRA --> Adapter --> Ollama
```
---
## ğŸ—ï¸ Execution Sequence (End-to-End)
```mermaid
sequenceDiagram
    participant D as Policy PDF
    participant S as Synthetic Generator
    participant Q as Data Quality Gate
    participant T as Trainer (PEFT)
    participant O as Ollama
    participant U as User

    D->>S: Parse + chunk policy
    S->>S: Generate synthetic Q/A (70% answerable, 30% unanswerable)
    S->>Q: Pass instruction dataset
    Q->>Q: Heuristic validation
    Q->>Q: LLM-based judging
    Q-->>T: High-quality instruction set
    T->>T: Fine-tune via LoRA PEFT
    T-->>O: Export adapter
    U->>O: Ask policy question
    O-->>U: Grounded answer / refusal
```

---

## ğŸ“ Project Structure
```text
domain_adaptive_llm_finetuning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ instructionquality.json           # Filtered training dataset
â”‚   
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ model.safetensors                 # Trained LoRA adapter
â”‚   â””â”€â”€ adapter_config.json
â”‚
â”œâ”€â”€ syntheticdatageneration.py            # Policy-constrained instruction generation
â”œâ”€â”€ preprocessing.py                      # Flatten + normalize instructions
â”œâ”€â”€ dataquality.py                        # Production-grade data quality gate
â”œâ”€â”€ train.py                              # PEFT fine-tuning script
â”‚
â”œâ”€â”€ generated_prompt.py                   # Instruction synthesis prompt
â”œâ”€â”€ Modelfile                             # Ollama adapter deployment config
â”‚
â”œâ”€â”€ pyproject.toml                        # uv dependency config
â”œâ”€â”€ uv.lock                               # Fully reproducible lockfile
â”œâ”€â”€ README.md                             # This file
```
---
## ğŸ”„ End-to-End Pipeline (From PDF to Specialized Model)

---

### 1ï¸âƒ£ Synthetic Data Generation

1. Parses a ***27-page HR policy PDF****
2. Uses **prompt constraints** to enforce:
     - no external knowledge
     - explicit uncertainty handling
     - numeric / system-specific questions
3. Produces 356 instructionâ€“response pairs

---

### 2ï¸âƒ£ Data Quality Gating

1. A production-grade filter, not a naive scorer.
2. Fast heuristics preserve:
     - short factual answers (e.g., â€œCAPPSâ€, â€œ15 minutesâ€)
     - correct â€œNot specifiedâ€ responses
3. LLM judge removes:
     - unrelated answers
     - malformed questions
     - weak supervision
4. Retains 240 high-quality samples (~37% reduction)

---

### 3ï¸âƒ£ Fine-Tuning (PEFT)

1. Base model: Llama-3.2-1B
2. Method: LoRA PEFT
     - rank = 64
     - alpha = 128
3. Precision: 4-bit quantization
5. Memory footprint: <2 GB GPU
6. Result: Behavioral specialization, not memorization

---

### 4ï¸âƒ£ Evaluation

1. Side-by-side testing against base model
2. Verified:
     - reduced hallucinations
     - correct refusal behavior
     - higher precision on policy-specific questions
3. Demonstrated clear divergence in next-token distributions

---

### 5ï¸âƒ£ Deployment

1. Exported LoRA adapter only (no full weights)
2. Deployed locally using Ollama
3. Runtime artifact: ~1.4 GB
4. Enables:
     - fast iteration
     - eproducible testing
     - base vs fine-tuned comparison

---

## ğŸ› ï¸ Prerequisites

### Local Development
- **Python 3.11+**
- **[`uv`](https://github.com/astral-sh/uv)** â€“ fast Python package & environment manager
- **Git**
- **Ollama (for local inference)**
  
### Training (Optional GPU)
- **RunPod / similar GPU VM**
- **CUDA-compatible GPU**
- **Hugging Face access (for base model)**

---
## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/domain_adaptive_llm_finetuning.git
cd domain_adaptive_llm_finetuning

uv venv
source .venv/bin/activate
uv sync
```

### 2ï¸âƒ£ Generate Data

```bash
uv run python syntheticdatageneration.py
uv run python preprocessing.py
uv run python dataquality.py
```


### 3ï¸âƒ£ Train

```bash
uv run python train.py
```
### 4ï¸âƒ£ Deploy Adapter Locally

```bash
ollama create llama_tuned -f Modelfile
ollama run llama_tuned
```





